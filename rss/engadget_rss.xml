<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:ss="http://contenthub.aol.com/slideshow/"><channel><title><![CDATA[Engadget is a web magazine with obsessive daily coverage of everything new in gadgets and consumer electronics]]></title><link/> https://www.engadget.com/ <description><![CDATA[Engadget is a web magazine with obsessive daily coverage of everything new in gadgets and consumer electronics]]></description><language> en-US</language><copyright>版权所有雅虎 2024</copyright><pubDate> Tue, 14 May 2024 18:04:13 +0000</pubDate><generator>雅虎 http://yahoo.com </generator><item><title><![CDATA[Google builds Gemini right into Android, adding contextual awareness within apps]]></title><description type="html"><![CDATA[<p>作为该<a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/how-to-watch-googles-io-2024-keynote-160010787.html">公司 I/O 2024 活动的</a>一部分，谷歌刚刚宣布对其适用于 Android 设备的 Gemini AI 聊天机器人<a data-i13n="cpos:1;pos:1" href="https://blog.google/products/android/google-ai-android-update-io-2024/">进行一些巧妙的改进</a>。 AI现已成为Android操作系统的一部分，使其能够以更全面的方式集成。</p><p>如果没有与底层操作系统的集成，最酷的新功能就不可能实现。当您控制智能手机上的应用程序时，双子座现在可以更好地理解上下文。这究竟意味着什么？一旦该工具作为 Android 15 的一部分正式启动，您将能够在您正在使用的应用程序之上调出 Gemini 覆盖层。这将允许执行特定于上下文的操作和查询。 </p><span id="end-legacy-contents"></span><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/1921d4a0-1153-11ef-a7af-c69fa951a0da" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/1921d4a0-1153-11ef-a7af-c69fa951a0da" style="height:768px;width:1510px;" alt="人工智能在行动。" data-uuid="52b44123-53d2-3d9e-aedc-f33852560a37"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p>Google 给出了快速将生成的图像放入 Gmail 和 Google Messages 中的示例，尽管您<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/google-pauses-geminis-ability-to-generate-people-after-overcorrecting-for-diversity-in-historical-images-220303074.html">现在可能希望避开历史图像</a>。该公司还推出了一项名为“询问此视频”的功能，让用户可以针对特定的 YouTube 视频提出问题，聊天机器人应该能够回答这些问题。</p><p>很容易看出这项技术的发展方向。一旦 Gemini 能够访问您的应用程序库的大部分，它应该能够真正兑现 Humane 和<a data-i13n="cpos:5;pos:1" href="https://www.engadget.com/rabbit-r1-review-a-199-ai-toy-that-fails-at-almost-everything-161043050.html">Rabbit</a> <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/the-humane-ai-pin-is-the-solution-to-none-of-technologys-problems-120002469.html">等人工智能竞争对手</a>做出的一些崇高承诺。谷歌表示，它“刚刚开始研究设备上的人工智能如何改变你的手机的功能”，因此我们想象未来至少会与 Uber 和 Doordash 等应用程序集成。</p><p>由于机载人工智能，Circle to Search 也得到了提升。用户将能够圈出手机上的几乎所有内容并接收相关信息。谷歌表示，人们无需切换应用程序即可做到这一点。这甚至延伸到数学和物理问题，只是圈出答案，这可能会让学生高兴而让老师感到沮丧。</p>本文最初发表在 Engadget 上：https://www.engadget.com/google-builds-gemini-right-into-android-adding-contextual-awareness-within-apps-180413356.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/google-builds-gemini-right-into-android-adding-contextual-awareness-within-apps-180413356.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 32f11a48-235a-4c8b-8021-6edbe3fc46bd</guid><dc:creator><![CDATA[Lawrence Bonk]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 18:04:13 +0000</pubDate><ingested> 1715709853</ingested><modified> 2024-05-14T18:04:18+00:00 </modified><category><![CDATA[Software]]></category><category><![CDATA[Mobile Apps]]></category><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[Information Technology]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Lawrence Bonk]]></category><media:content height="783" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2Fbb2884c0-1152-11ef-9f3f-f2f271569d33&amp;resize=1400%2C783&amp;client=19f2b5e49a271b2bde77&amp;signature=b49002c1732da9e41525ed15d86f1d9f44c09262" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="783" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2Fbb2884c0-1152-11ef-9f3f-f2f271569d33&amp;resize=1400%2C783&amp;client=19f2b5e49a271b2bde77&amp;signature=6375a078f57b111582da96708a5eac3799482a4a" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[An image of the new Gemini for Android.]]></media:description><media:title><![CDATA[Google Gemini]]></media:title></media:content></item><item><title><![CDATA[Android's Circle to Search can now help students solve math and physics homework]]></title><description type="html"><![CDATA[<p>谷歌在该公司的年度 I/O 开发者大会上推出了<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/googles-circle-to-search-feature-will-soon-handle-language-translation-174802558.html">另一项“</a> Circle to Search”功能，它可以帮助学生<a data-i13n="cpos:2;pos:1" href="https://blog.google/products/android/google-ai-android-update-io-2024">更好地理解潜在的困难课程主题</a>。该功能现在将能够向他们展示“一系列物理和数学应用题”的分步说明。他们只需长按主页按钮或导航栏，然后圈出让他们感到困惑的问题即可激活该功能，尽管有些数学问题需要用户注册谷歌的实验性搜索实验室功能。</p><p>该公司表示，Circle to Search 的新功能是通过其名为 LearnLM 的新人工智能模型家族实现的，该模型是专门为学习而创建和微调的。它还计划对这一特定功能进行调整，并在今年晚些时候推出升级版本，可以解决更复杂的问题，“涉及符号公式、图表等”。谷歌今年早些时候在三星 Unpacked 活动上<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/galaxy-s24-and-pixel-8-owners-can-soon-search-for-anything-by-drawing-a-circle-around-it-180029757.html">推出了 Circle to Search</a> ，因为该功能最初在 Galaxy 24 和 Pixel 8 设备上可用。它现在也适用于 Galaxy S23、Galaxy S22、Z Fold、Z Flip、Pixel 6 和 Pixel 7 设备，并且未来可能会应用于更多硬件。</p><span id="end-legacy-contents"></span><p>除了新的 Circle to Search 功能外，谷歌还透露，支持 Android 聊天机器人助手 Gemini 的设备现在可以将其作为覆盖层显示在当前打开的应用程序之上。然后，用户可以将图像直接从叠加层拖放到 Gmail 等应用程序中，或者使用叠加层查找信息，而无需从他们正在做的事情上滑开。他们可以点击“询问此视频”以在打开的 YouTube 视频中查找特定信息，如果他们有权访问 Gemini Advanced，则可以使用“询问此 PDF”选项从冗长的文档中查找信息。</p><p>谷歌还向 Nano 推出了多模式功能，Nano 是 Gemini 系列中最小的型号，可以在设备上处理信息。更新后的 Gemini Nano 将能够处理视觉、声音和口语，将于今年晚些时候出现在 Google 的 TalkBack 屏幕阅读器中。 Gemini Nano 将使 TalkBack 能够更快地描述屏幕上的图像，甚至在没有互联网连接的情况下也是如此。最后，谷歌目前正在测试 Gemini Nano 功能，如果检测到与诈骗相关的常见对话模式，该功能可以在通话过程中向用户发出警报。例如，如果用户正在与要求他们提供 PIN 或密码的人交谈，或者与要求他们购买礼品卡的人交谈，则会收到警报。</p><p>在<a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/androids-circle-to-search-can-now-help-students-solve-math-and-physicals-homework-180223229.html?src=rss ]]>; </description><link/><![CDATA[https://www.engadget.com/androids-circle-to-search-can-now-help-students-solve-math-and-physics-homework-180223229.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 85d9d7dd-f8c7-4317-a50a-5f7319394d06</guid><dc:creator><![CDATA[Mariella Moon]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 18:02:23 +0000</pubDate><ingested> 1715709744</ingested><modified> 2024-05-14T18:02:36+00:00 </modified><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Mariella Moon]]></category><media:content height="889" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2Fa60abd90-11d6-11ef-bfe1-78dcd8ffbe7c&amp;resize=1400%2C889&amp;client=19f2b5e49a271b2bde77&amp;signature=7243f9263f1c4c0f53390b9cdc04329b9b6ee0eb" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="889" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2Fa60abd90-11d6-11ef-bfe1-78dcd8ffbe7c&amp;resize=1400%2C889&amp;client=19f2b5e49a271b2bde77&amp;signature=2baf8de57e04c36f248dbb743310c32a57d11538" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[Android circle to search feature.]]></media:description><media:title><![CDATA[Android circle to search]]></media:title></media:content></item><item><title><![CDATA[Google's Gemini will search your videos to help you solve problems]]></title><description type="html"><![CDATA[<p>作为推动将生成式人工智能添加到搜索中的一部分，谷歌引入了一个新的转折点：视频。 Gemini 会让您上传演示您要解决的问题的视频，然后搜索用户论坛和互联网的其他区域以找到解决方案。</p><p>举个例子，Google 的 Rose Yao 在 I/O 2024 的舞台上谈到了她购买的二手转盘以及她如何无法将唱针固定在唱片上。 Yao 上传了一段显示该问题的视频，然后 Gemini 很快找到了一个解释者，描述了如何在特定品牌和型号上平衡手臂。 </p><span id="end-legacy-contents"></span><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/7c7a9110-1183-11ef-ab8f-70abbde0846d" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/7c7a9110-1183-11ef-ab8f-70abbde0846d" style="height:1000px;width:1000px;" alt="谷歌的 Gemini 现在搜索视频来回答你的问题" data-uuid="e2a63ebf-db5c-3f12-b76f-0419bb7cfc9c"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p>谷歌写道：“搜索不仅仅是文本框中的文字。你遇到的问题通常是关于你周围看到的事物，包括运动的物体。” “通过视频进行搜索可以节省您寻找正确词语来描述此问题的时间和麻烦，并且您将获得包含故障排除步骤和资源的 AI 概述。”</p><p>如果仅靠视频无法清楚地说明您想要弄清楚的内容，您可以添加文本或绘制指向相关问题的箭头。</p><p> OpenAI <a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/openai-claims-that-its-free-gpt-4o-model-can-talk-laugh-sing-and-see-like-a-human-184249780.html">刚刚推出了 ChatGPT 4o</a> ，它能够实时解释实时视频，然后描述一个场景，甚至唱一首与之相关的歌曲。然而，谷歌在视频方面采取了不同的策略，目前专注于其搜索产品。该公司表示，美国搜索实验室用户首先可以使用英语进行视频搜索，但随着时间的推移，将扩展到更多地区。</p><p>在<a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/googles-gemini-will-search-your-videos-to-help-you-solve-problems-175235105.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/googles-gemini-will-search-your-videos-to-help-you-solve-problems-175235105.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 64f4e9e3-35cd-4ad5-9cc3-d0ff75ffc5f9 </guid><dc:creator><![CDATA[Steve Dent]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:52:35 +0000</pubDate><ingested> 1715709156</ingested><modified> 2024-05-14T17:52:44+00:00 </modified><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Steve Dent]]></category><media:content height="994" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F15423330-1184-11ef-a0ff-9574304716fb&amp;resize=1400%2C994&amp;client=19f2b5e49a271b2bde77&amp;signature=2690fc0af2c66e8b0aa2f2342d5c2230e795dfe6" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="994" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2Fe7e15230-1189-11ef-beff-019ee3a658ed&amp;resize=1400%2C994&amp;client=19f2b5e49a271b2bde77&amp;signature=cbd3140f4f88893ff730d1aea2074733e51c40cc" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[Google's Gemini will search video to answer your questions]]></media:description><media:title><![CDATA[Google's Gemini will search video to answer your questions]]></media:title></media:content></item><item><title><![CDATA[Google expands digital watermarks to AI-made video]]></title><description type="html"><![CDATA[<p>随着谷歌开始提供最新的视频生成工具，该公司表示，它有一项计划，以确保其日益逼真的人工智能生成剪辑的来源透明。得益于 Google 的 SynthID 系统，VideoFX 应用程序中该公司新 Veo 模型制作的所有视频都将带有数字水印。</p><p> SynthID 是谷歌的<a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:1;pos:1" class="no-affiliate-link" href="https://deepmind.google/technologies/synthid/" data-original-link="https://deepmind.google/technologies/synthid/">数字水印</a>系统， <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1" class="no-affiliate-link" href="https://www.engadget.com/google-wants-an-invisible-digital-watermark-to-bring-transparency-to-ai-art-164551794.html" data-original-link="https://www.engadget.com/google-wants-an-invisible-digital-watermark-to-bring-transparency-to-ai-art-164551794.html">去年</a>开始推出人工智能生成的图像。该技术将难以察觉的水印嵌入到人工智能制作的内容中，以便人工智能检测工具可以识别出该内容是由人工智能生成的。考虑到该公司在 I/O 舞台上预览的最新视频生成模型 Veo 可以创建比以前更长、更高分辨率的剪辑，因此跟踪此类内容的来源将变得越来越重要。</p><span id="end-legacy-contents"></span><p> DeepMind 首席执行官 Demis Hassabis 在接受记者采访时表示，SynthID 水印也将扩展到人工智能生成的文本。随着生成式人工智能模型的进步，由于担心人工智能可能助长新一波的错误信息，越来越多的公司开始转向水印。水印系统将为谷歌等平台提供一个框架，用于检测人工智能生成的内容，否则这些内容可能无法区分。 <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:3;pos:1" class="no-affiliate-link" href="https://www.engadget.com/tiktok-will-automatically-label-more-ai-generated-content-in-its-app-120001090.html" data-original-link="https://www.engadget.com/tiktok-will-automatically-label-more-ai-generated-content-in-its-app-120001090.html">TikTok</a>和<a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:4;pos:1" class="no-affiliate-link" href="https://www.engadget.com/meta-plans-to-more-broadly-label-ai-generated-content-152945787.html" data-original-link="https://www.engadget.com/meta-plans-to-more-broadly-label-ai-generated-content-152945787.html">Meta</a>最近还宣布计划在其平台上支持类似的检测工具，并在其应用程序中标记更多人工智能内容。</p><p>当然，数字水印本身是否能提供足够的保护以防止欺骗性人工智能内容仍然存在<a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:5;pos:1" class="no-affiliate-link" href="https://www.engadget.com/can-digital-watermarking-protect-us-from-generative-ai-184542396.html" data-original-link="https://www.engadget.com/can-digital-watermarking-protect-us-from-generative-ai-184542396.html">重大问题</a>。研究人员已经证明水印很容易被<a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:6;pos:1" class="no-affiliate-link" href="https://www.engadget.com/researchers-say-current-ai-watermarks-are-trivial-to-remove-204414059.html" data-original-link="https://www.engadget.com/researchers-say-current-ai-watermarks-are-trivial-to-remove-204414059.html">规避</a>。但以某种方式检测人工智能制作的内容是迈向透明度的重要的第一步。</p><p>在<a data-i13n="cpos:7;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/google-expands-digital-watermarks-to-ai-made-video-175232320.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/google-expands-digital-watermarks-to-ai-made-video-175232320.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 049c53a2-f09b-4710-bc63-ecc8759a3778 </guid><dc:creator><![CDATA[Karissa Bell]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:52:32 +0000</pubDate><ingested> 1715709153</ingested><modified> 2024-05-14T17:52:46+00:00 </modified><category><![CDATA[Software]]></category><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[Information Technology]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Karissa Bell]]></category><media:content height="778" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F2f2d2350-1183-11ef-9bee-68d609545e82&amp;resize=1400%2C778&amp;client=19f2b5e49a271b2bde77&amp;signature=ab034557e01a3903e3f848c3c44969b97c3901f1" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="778" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F2f2d2350-1183-11ef-9bee-68d609545e82&amp;resize=1400%2C778&amp;client=19f2b5e49a271b2bde77&amp;signature=0fece10319220d381738e0d3bcf2cf80b40a6a2c" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[Google's SynthID watermarks are coming to AI-gen video.]]></media:description><media:title><![CDATA[SynthID]]></media:title></media:content></item><item><title><![CDATA[Google Search will now show AI-generated answers to millions by default]]></title><description type="html"><![CDATA[<p>谷歌正在改变搜索。周二，该公司在谷歌开发者年度大会 I/O 上宣布了对全球主导搜索引擎进行人工智能驱动的重大变革。凭借这些新功能，谷歌将搜索定位为不仅仅是一种简单地查找网站的方式。相反，该公司希望人们使用其搜索引擎直接获得答案，并帮助他们规划活动和集思广益。</p><p> “借助生成式人工智能，搜索可以做的事情比你想象的还要多，”谷歌搜索副总裁兼负责人 Liz Reid 在一篇<a data-i13n="cpos:1;pos:1" href="https://blog.google/products/search/generative-ai-google-search-may-2024">博客文章</a>中写道。 “所以你可以提出任何你想到的事情或任何你需要完成的事情——从研究到计划再到集思广益——谷歌将负责所有的跑腿工作。”</p><span id="end-legacy-contents"></span><p>谷歌对搜索（该公司赚钱的主要方式）的改变是对自 2022 年底 OpenAI 的 ChatGPT 发布以来生成式人工智能爆炸式增长的回应。从那时起，包括 ChatGPT、Anthropic 在内的一些人工智能驱动的应用程序和服务出现了。 、Perplexity 和由 OpenAI 的 GPT-4 提供支持的微软 Bing 通过直接提供问题答案而不是简单地向人们提供链接列表来挑战谷歌的旗舰服务。这就是谷歌正在竞相通过其搜索新功能来弥补的差距。</p><p>从今天开始，谷歌将在美国结果页面的顶部显示人工智能生成的完整答案，以响应大多数搜索查询。谷歌在一年前的 2023 年 Google I/O 大会上<a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/google-search-generative-experience-preview-a-familiar-yet-different-approach-175156245.html?_fsig=Jq96xGV4zAtGSgsHELLUaA--~A">首次推出了</a>该功能，但到目前为止，任何想要使用该功能的人都必须注册该功能，作为该公司<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/google-will-start-showing-ai-powered-search-results-to-users-who-didnt-opt-in-093036257.html">搜索实验室</a>平台的一部分，该平台允许人们在发布之前试用即将推出的功能。他们的普遍释放。谷歌目前正在向数亿美国人提供人工智能概述，并表示预计到今年年底将在更多国家向超过 10 亿人提供该服务。里德写道，选择通过搜索实验室尝试该功能的人到目前为止已经使用了“数十亿次”，并表示，作为人工智能生成的答案的一部分包含的任何链接都会比该页面作为传统页面出现时获得更多的点击次数。网络列表，这是出版商一直关心的问题。 “随着我们扩展这种体验，我们将继续专注于向出版商和创作者发送有价值的流量，”里德写道。</p><p>除了人工智能概述之外，在美国用英语搜索有关餐饮和食谱的某些查询，以及随后搜索电影、音乐、书籍、酒店、购物等内容，将显示一个新的搜索页面，其中使用人工智能来组织结果。 “当你在寻找想法时，搜索将使用生成人工智能与你进行头脑风暴，并创建一个人工智能组织的结果页面，使探索变得容易，”里德在博客文章中说。 </p><p></p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/87302290-11bb-11ef-befd-63da25cb3455" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/87302290-11bb-11ef-befd-63da25cb3455" style="height:750px;width:750px;" alt="AI 组织结果页面" data-uuid="b13c1389-8ba8-3ec8-936d-24bdc45089a5"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p></p><p>如果您选择加入搜索实验室，您将能够在 Google 搜索中访问由生成式 AI 提供支持的更多功能。您将能够获得人工智能概述来简化语言或更详细地分解复杂的主题。这是一个要求 Google 解释的查询示例，例如闪电和雷声之间的联系。 </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/3d6fac20-11bb-11ef-87bd-16f612799e13" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/3d6fac20-11bb-11ef-87bd-16f612799e13" style="height:623px;width:960px;" alt="Google 搜索中的一项新功能可让人们提出复杂的查询" data-uuid="228a9960-758f-38b8-bdec-4fb911275afa"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p></p><p></p><p>搜索实验室测试人员还可以在单​​个查询中向 Google 提出非常复杂的问题，以便在单个页面上获得答案，而不必进行多次搜索。谷歌博客文章给出的例子是：“找到波士顿最好的瑜伽或普拉提工作室，并显示他们的介绍优惠和从灯塔山出发的步行时间的详细信息。”作为回应，谷歌展示了波士顿灯塔山附近评价最高的瑜伽和普拉提工作室，甚至将它们放在地图上以便于导航。 </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/b1e29db0-11bb-11ef-bb2c-4726913964d5" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/b1e29db0-11bb-11ef-bb2c-4726913964d5" style="height:750px;width:750px;" alt="灯塔山" data-uuid="69192499-fc9d-30bd-80ee-4063c2e87e51"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p>谷歌还希望成为膳食和假期规划者，让注册搜索实验室的人提出诸如“为易于准备的团体创建 3 天膳食计划”之类的问题，并让你在人工智能生成的计划中交换个人结果用其他东西（例如，将膳食计划中的肉类菜肴改为素食菜肴）。 </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/d62cc6f0-11bb-11ef-bdbd-d8fbd6988273" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/d62cc6f0-11bb-11ef-bdbd-d8fbd6988273" style="height:623px;width:960px;" alt="膳食计划" data-uuid="9933ba94-221e-30c7-936c-373f47fc6595"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p>最后，谷歌最终将允许任何注册搜索实验室的人使用视频作为搜索查询，而不是文本或图像。里德在谷歌的博客文章中写道：“也许你在旧货店买了一台电唱机，但当你打开它时，它无法工作，并且带有唱针的金属片会意外地漂移。” “使用视频搜索可以节省您寻找正确词语来描述此问题的时间和麻烦，并且您将获得人工智能概述，其中包含用于排除故障的步骤和资源。”</p><p>谷歌表示，所有这些新功能均由专为搜索定制的全新Gemini模型提供支持，该模型将Gemini先进的多步推理和多模态能力与谷歌的传统搜索系统相结合。</p><p>在<a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p><p></p>本文最初发表在 Engadget 上，网址为 https://www.engadget.com/google-search-will-now-show-ai- generated-answers-to-millions-by-default-174512845.html?src=rss]] >; </description><link/><![CDATA[https://www.engadget.com/google-search-will-now-show-ai-generated-answers-to-millions-by-default-174512845.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> a7131772-8493-46b2-9be4-d53623feb717 </guid><dc:creator><![CDATA[Pranav Dixit]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:45:12 +0000</pubDate><ingested> 1715708713</ingested><modified> 2024-05-14T17:45:30+00:00 </modified><category><![CDATA[Media]]></category><category><![CDATA[Personal Investing Ideas & Strategies]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Pranav Dixit]]></category><media:content height="933" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F13a21d50-11bc-11ef-b65e-4c89bb8bc596&amp;resize=1400%2C933&amp;client=19f2b5e49a271b2bde77&amp;signature=474f1e053dba12de828b79a71caf5a032a73b02b" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="933" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F13a21d50-11bc-11ef-b65e-4c89bb8bc596&amp;resize=1400%2C933&amp;client=19f2b5e49a271b2bde77&amp;signature=6687d930aa059069bbd42dc687c324e53843a2d6" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[SOPA Images via Getty Images]]></media:credit><media:description><![CDATA[BRAZIL - 2024/02/28: In this photo illustration, the Google IO logo is displayed on a smartphone screen. (Photo Illustration by Rafael Henrique/SOPA Images/LightRocket via Getty Images)]]></media:description><media:title><![CDATA[In this photo illustration, the Google IO logo is displayed...]]></media:title></media:content></item><item><title><![CDATA[Google unveils Veo and Imagen 3, its latest AI media creation models]]></title><description type="html"><![CDATA[<p> Google I/O 上一直都是人工智能！今天，谷歌发布了新的人工智能媒体创作引擎：Veo，可以制作“高质量”1080p 视频；以及 Imagen 3，其最新的文本到图像框架。这两个听起来都不是特别革命性的，但它们是谷歌继续对抗<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/openais-new-sora-model-can-generate-minute-long-videos-from-text-prompts-195717694.html">OpenAI 的 Sora 视频模型</a>和<a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/the-next-dall-e-will-be-able-to-generate-results-within-chatgpt-063833672.html">Dall-E 3 的一种方式，Dall-E 3</a>实际上已经成为人工智能生成图像的代名词。</p><p>谷歌声称 Veo 具有“对自然语言和视觉语义的高级理解”，可以创建您想要的任何视频。人工智能生成的视频可以持续“超过一分钟”。 Veo 还能够理解电影和视觉技术，例如延时拍摄的概念。但实际上，这应该是人工智能视频生成模型的赌注，对吗？ </p><span id="end-legacy-contents"></span><div id="2be2fe13371a493ead9c5f63beb941a8"><iframe src="https://www.youtube.com/embed/dKAVFLB75xs?rel=0" style="top:0;left:0;width:100%;height:100%;position:absolute;border:0;" allowfullscreen="" scrolling="no" data-embed-domain="www.youtube.com"></iframe></div><p>为了证明 Veo 不会抢走艺术家的工作，谷歌还与 Donald Glover 和他的创意工作室 Gilga 合作，展示该模型的功能。在一段非常简短的宣传视频中，我们看到格洛弗和工作人员用文字制作了一辆敞篷车抵达欧洲家庭、一艘帆船在海洋中滑行的视频。据谷歌称，Veo 可以比之前的模型更好地模拟现实世界的物理现象，并且还改进了渲染高清镜头的方式。</p><p> “每个人都会成为导演，每个人都应该成为导演，”格洛弗在视频中说道，他的薪水绝对是靠谷歌赚来的。 “这一切的核心就是讲故事。我们越能互相讲述我们的故事，我们就越能理解彼此。”</p><p>除了看到机器试图通过算法重现人类艺术家作品的病态好奇心之外，是否有人真的想观看人工智能生成的视频还有待观察。但这并不能阻止 Google 或 OpenAI 推广这些工具并希望它们有用（或者至少能赚大钱）。 Veo 今天将在 Google VideoFX 工具中为一些创作者提供，该公司表示它也将出现在 YouTube Shorts 和其他产品中。如果 Veo 最终成为 YouTube Shorts 的内置部分，那么这至少是谷歌可以超越 TikTok 的一项功能。 </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/5388e411-1218-11ef-96a4-a8a281e91461" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/5388e411-1218-11ef-96a4-a8a281e91461" style="height:1080px;width:1920px;" alt="谷歌IO 2024" data-uuid="c88033e6-bf36-3db2-adac-ff351dac33c3"><figcaption></figcaption><div class="photo-credit">谷歌</div></figure><p>至于 Imagen 3，谷歌做出了一贯的承诺：据说这是该公司“最高质量”的文本到图像模型，具有“令人难以置信的细节水平”，可实现“逼真、逼真的图像”，并且伪影更少。当然，真正的测试是看看它与 Dall-E 3 相比如何处理提示。谷歌表示，Imagen 3 处理文本的能力比以前更好，而且在处理长提示的细节方面也更智能。</p><p>谷歌还与 Wyclef Jean 和 Bjorn 等唱片艺术家合作测试其 Music AI Sandbox，这是一套可以帮助歌曲和节拍创作的工具。我们只看到了这一点，但它带来了一些有趣的演示： </p><div id="e4397b94971b435ca6ac267f8f25224f"><iframe src="https://www.youtube.com/embed/videoseries?list=PLqYmG7hTraZA7o7KkLWoVscoELWRGu3Xg" style="top:0;left:0;width:100%;height:100%;position:absolute;border:0;" allowfullscreen="" scrolling="no" data-embed-domain="www.youtube.com"></iframe></div><p>太阳升起和落下。我们都在慢慢死去。人工智能正变得越来越聪明。这似乎是谷歌最新媒体创建工具的一大收获。当然，他们正在变得更好！谷歌正在投入数十亿美元来实现人工智能的梦想，所有这些都是为了实现计算领域的下一次巨大飞跃。这真的会让我们的生活变得更好吗？他们能否创作出具有真正灵魂的艺术？每年都会回顾一下 Google I/O，直到 AGI 真正出现，或者我们的文明崩溃。</p><p><em>发展...</em></p><p>在<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p><p></p>本文最初发表在 Engadget 上：https://www.engadget.com/google-unveils-veo-and-imagen-3-its-latest-ai-media-creation-models-173617373.html?src=rss]] >; </description><link/><![CDATA[https://www.engadget.com/google-unveils-veo-and-imagen-3-its-latest-ai-media-creation-models-173617373.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 7819e262-243c-42e9-ac30-f6abee01197a </guid><dc:creator><![CDATA[Devindra Hardawar]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:36:17 +0000</pubDate><ingested> 1715708867</ingested><modified> 2024-05-14T17:48:00+00:00 </modified><category><![CDATA[Media]]></category><category><![CDATA[Arts & Entertainment]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Devindra Hardawar]]></category><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F538895f0-1218-11ef-a6ff-66e559369394&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=b8487d4772ce548d0425d49ad8910a8d5da42881" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F538895f0-1218-11ef-a6ff-66e559369394&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=393a37cea38177b7e8d8cf32f17ba3d88d8ba3c8" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[Google IO 2024]]></media:description><media:title><![CDATA[Google IO 2024]]></media:title></media:content></item><item><title><![CDATA[Google just snuck a pair of AR glasses into a Project Astra demo at I/O]]></title><description type="html"><![CDATA[<p>在一段展示其新 Project Astra 应用程序强大功能的视频中，演示者问 Gemini：“你还记得在哪里看到我的眼镜吗？”人工智能令人印象深刻地回答“是的，我愿意。你的眼镜在桌子上靠近一个红苹果的地方”，尽管在提出问题时实际上并没有看到该物体。但这些并不是沼泽标准的视觉辅助工具。这些眼镜上有一个摄像头和某种视觉界面！</p><p>测试人员拿起眼镜戴上，然后向人工智能询问更多有关他们所看到的事物的问题。显然，设备上有一个摄像头可以帮助它捕捉周围的环境，我们看到了某种界面，其中波形移动以表明它正在监听。屏幕上的字幕似乎也反映了向佩戴者大声朗读的答案。因此，如果我们继续跟踪的话，至少还会有一个麦克风和扬声器，以及某种处理器和电池来为整个设备供电。</p><span id="end-legacy-contents"></span><p>我们只瞥见了这款可穿戴设备的一瞥，但从它出现的那几秒钟来看，有一些事情是显而易见的。这款眼镜有一个简单的黑色镜框，看起来一点也不像谷歌眼镜。它们看起来也不是很笨重。</p><p>谷歌很可能还没有准备好在 I/O 大会上实际推出一副眼镜。它轻而易举地跳过了可穿戴设备的外观，几乎没有提及它们，只是说 Project Astra 和该公司的“通用代理”愿景可能会出现在我们的手机或眼镜等设备上。目前我们还不太了解，但如果您一直在哀悼谷歌眼镜或该公司其他失败的可穿戴产品，这可能会给您带来一些希望。</p><p>在<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/google-just-snuck-a-pair-of-ar-glasses-into-a-project-astra-demo-at-io-172824539.html ?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/google-just-snuck-a-pair-of-ar-glasses-into-a-project-astra-demo-at-io-172824539.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> e1b6cc14-86fe-4ccb-887b-1cea99577c4e </guid><dc:creator><![CDATA[Cherlynn Low]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:28:24 +0000</pubDate><ingested> 1715707705</ingested><modified> 2024-05-14T17:28:34+00:00 </modified><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[Information Technology]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Cherlynn Low]]></category><media:content height="836" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2Fbbd5f600-1206-11ef-bfe6-52528ccdad12&amp;resize=1400%2C836&amp;client=19f2b5e49a271b2bde77&amp;signature=89274b88ef6fcb76b78d6ad304faef903b9ae423" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="836" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2Fbbd5f600-1206-11ef-bfe6-52528ccdad12&amp;resize=1400%2C836&amp;client=19f2b5e49a271b2bde77&amp;signature=8217ad7f35a74f01b5edee2dd9e5a0f628e10bb6" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google (Screenshot)]]></media:credit><media:description><![CDATA[A screenshot from a video showing off Project Astra, with an inset on the bottom right containing a person wearing a pair of black-framed glasses.]]></media:description><media:title><![CDATA[Project Astra glasses screenshot]]></media:title></media:content></item><item><title><![CDATA[Google's Project Astra uses your phone's camera and AI to find noise makers, misplaced items and more.]]></title><description type="html"><![CDATA[<p>当谷歌在 2018 年的开发者大会上<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/2018-05-08-google-assistant-will-soon-be-able-to-make-calls-to-book-your-ap.html">首次展示其 Duplex 语音助手</a>技术时，既令人印象深刻又令人担忧。今天，在 I/O 2024 上，该公司可能会再次引发同样的反应，这次是通过名为 Project Astra 的项目展示其人工智能智能的另一种应用。</p><p>该公司甚至迫不及待地等到今天的主题演讲来调侃 Project Astra， <a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/google-teases-new-camera-powered-ai-feature-one-day-ahead-of-io-175452903.html">昨天在其社交媒体上发布了一段基于摄像头的人工智能应用程序</a>的视频。不过，在今天的主题演讲中，谷歌 DeepMind 首席执行官 Demis Hassabis 表示，他的团队“一直希望开发对日常生活有帮助的通用人工智能代理”。 Project Astra 是这方面进展的结果。</p><span id="end-legacy-contents"></span><h2 id="jump-link-what-is-project-astra">什么是阿斯特拉计划？</h2><p>根据谷歌昨天在媒体吹风会上展示的一段视频，Project Astra 似乎是一款以取景器作为主界面的应用程序。一个人拿着手机，将摄像头对准办公室的各个地方，口头说道：“当你看到有东西发出声音时，请告诉我。”当显示器旁边的扬声器出现时，双子座回答“我看到一个扬声器，它发出声音。”</p><p>电话后面的人停下来，在屏幕上画了一个箭头到扬声器顶部的圆圈，然后说：“扬声器的那部分叫什么？”双子座立即回答：“那是高音扬声器，它发出高频声音。” </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/e5726d40-1206-11ef-afe6-5fa9a2f1c6d9"><figcaption></figcaption></figure><p>然后，在谷歌所说的一次性录制的视频中，测试人员走到桌子下方的一杯蜡笔前，询问“给我一个关于这些的创意头韵”，双子座说“创意蜡笔颜色愉快。”他们当然会创作出丰富多彩的作品。”</p><h2 id="jump-link-wait-were-those-project-astra-glasses-is-google-glass-back">等等，那些是 Project Astra 眼镜吗？谷歌眼镜回来了吗？</h2><p>视频的其余部分继续显示 Project Astra 中的 Gemini 识别并解释监视器上的部分代码，根据窗外的景色告诉用户他们所在的社区。最令人印象深刻的是，阿斯特拉能够回答“你还记得在哪里看到我的眼镜吗？”尽管所说的眼镜完全脱离镜框并且之前没有指出。 “是的，我愿意，”双子座说，并补充道，“你的眼镜放在桌子上，靠近一个红苹果。”</p><p> Astra 找到这些眼镜后，测试人员戴上它们，视频就会切换到您在可穿戴设备上看到的视角。眼镜使用机载摄像头扫描佩戴者的周围环境，以查看白板上的图表等内容。视频中的人接着问道：“我可以在这里添加什么来让这个系统更快？”当他们说话时，屏幕上的波形移动以表明它正在聆听，当它做出响应时，文本标题会同时出现。 Astra 表示“在服务器和数据库之间添加缓存可以提高速度。”</p><p>然后测试者看着黑板上涂鸦的一对猫并问道：“这让你想起了什么？”阿斯特拉说“薛定谔的猫”。最后，他们拿起一个毛绒老虎玩具，把它放在一只可爱的金毛猎犬旁边，并询问“这对组合的乐队名称”。阿斯特拉尽职尽责地回答：“金色条纹。”</p><h2 id="jump-link-how-does-project-astra-work">阿斯特拉计划如何运作？</h2><p>这意味着 Astra 不仅可以实时处理视觉数据，还可以记住所看到的内容并处理大量积压的存储信息。哈萨比斯表示，之所以能实现这一目标，是因为这些“代理”“旨在通过连续编码视频帧、将视频和语音输入组合到事件时间线中，并缓存这些信息以进行有效回忆，从而更快地处理信息。”</p><p>还值得注意的是，至少在视频中，Astra 的反应很快。哈萨比斯在一篇博客文章中指出，“虽然我们在开发能够理解多模式信息的人工智能系统方面取得了令人难以置信的进展，但将响应时间缩短为对话式的内容是一项艰巨的工程挑战。”</p><p>谷歌还一直致力于为其人工智能提供更广泛的声音表达，利用其语音模型“增强他们的声音，为代理提供更广泛的语调”。这种对人类表达能力的模仿让人想起 Duplex 的停顿和话语，这些停顿和话语让人们认为<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/2018-05-08-pretty-sure-googles-new-talking-ai-just-beat-the-turing-test.html">谷歌的人工智能可能是图灵测试的候选者</a>。</p><h2 id="jump-link-when-will-project-astra-be-available">阿斯特拉计划什么时候可用？</h2><p>虽然 Astra 仍然是一个早期功能，没有明显的发布计划，但哈萨比斯写道，未来这些助手可以“通过手机或眼镜”使用。目前还没有关于这些眼镜实际上是产品还是谷歌眼镜的后继者的消息，但哈萨比斯确实写道，“其中一些功能将在今年晚些时候出现在谷歌产品中，比如 Gemini 应用程序。”</p><p>在<a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/googles-project-astra-uses-your-phones-camera-and-ai-to-find-noise-makers-misplaced-items-and-more -172642329.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/googles-project-astra-uses-your-phones-camera-and-ai-to-find-noise-makers-misplaced-items-and-more-172642329.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 6ad638b2-e4b3-4e17-a51d-62e562c5b1fc </guid><dc:creator><![CDATA[Cherlynn Low]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:28:00 +0000</pubDate><ingested> 1715707681</ingested><modified> 2024-05-14T17:28:17+00:00 </modified><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[Small Businesses]]></category><category><![CDATA[Information Technology]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Cherlynn Low]]></category><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F44fb7e80-11ff-11ef-bfbe-572e6162313e&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=efcd001da2f6235b5ab3716d12e2b94982ee3957" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F44fb7e80-11ff-11ef-bfbe-572e6162313e&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=428f20424a0b6fab016cafc1525b4a239faf1bc9" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[A graphic with the words Project Astra with a series of overlapping circles in the background.]]></media:description><media:title><![CDATA[Google Project Astra]]></media:title></media:content></item><item><title><![CDATA[Google's new Gemini 1.5 Flash AI model is lighter than Gemini Pro and more accessible]]></title><description type="html"><![CDATA[<p>周二，谷歌在公司开发者年度大会 I/O 上宣布了其<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/googles-answer-to-gpt-4-is-gemini-the-most-capable-model-weve-ever-built-150039571.html">Gemini 系列</a>人工智能模型的更新。该公司正在推出一款名为 Gemini 1.5 Flash 的新型号，据称该型号针对速度和效率进行了优化。</p><p> “[Gemini] 1.5 Flash 擅长摘要、聊天应用程序、图像和视频字幕、从长文档和表格中提取数据等，”Google DeepMind 首席执行官 Demis Hassabis 在一篇<a data-i13n="cpos:2;pos:1" href="https://blog.google/technology/ai/google-gemini-update-ash-ai-assistant-io-2024">博客文章</a>中写道。 Hassabis 补充说，谷歌创建 Gemini 1.5 Flash 是因为开发人员需要一个比谷歌 2 月份<a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/googles-gemini-15-pro-is-a-new-more-efficient-ai-model-181909354.html">宣布的</a>Pro 版本更轻、更便宜的模型。 Gemini 1.5 Pro 比该公司去年年底发布的原始 Gemini 型号更高效、更强大。</p><span id="end-legacy-contents"></span><p> Gemini 1.5 Flash 介于 Gemini 1.5 Pro 和 Gemini 1.5 Nano 之间，后者是 Google 在设备上本地运行的最小型号。尽管重量比 Gemini Pro 轻，但它的功能同样强大。谷歌表示，这是通过一个名为“蒸馏”的过程实现的，将 Gemini 1.5 Pro 中最重要的知识和技能转移到较小的模型上。这意味着 Gemini 1.5 Flash 将获得与 Pro 相同的多模式功能，以及其长上下文窗口（AI 模型一次可以摄取的数据量）一百万个代币。据 Google 称，这意味着 Gemini 1.5 Flash 将能够同时分析 1,500 页的文档或超过 30,000 行的代码库。</p><p> Gemini 1.5 Flash（或任何这些型号）并不是真正适合消费者的。相反，对于开发人员来说，这是一种更快、更便宜的方式，可以使用谷歌设计的技术构建自己的人工智能产品和服务。</p><p>除了推出Gemini 1.5 Flash之外，谷歌还对Gemini 1.5 Pro进行了升级。该公司表示，它“增强”了该模型编写代码、推理以及解析音频和图像的能力。但最大的更新尚未到来——谷歌宣布今年晚些时候将模型的现有上下文窗口增加一倍，达到 200 万个代币。这将使其能够同时处理 2 小时的视频、22 小时的音频、超过 60,000 行代码或超过 140 万个单词。</p><p> Gemini 1.5 Flash 和 Pro 现已在 Google 的 AI Studio 和 Vertex AI 中提供公开预览版。该公司今天还宣布了 Gemma 开放模型的新版本，称为 Gemma 2。但是，除非您是开发人员或喜欢构建 AI 应用程序和服务的人，否则这些更新并不真正适合普通消费者。</p><p>在<a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p><p></p>本文最初发表在 Engadget 上：https://www.engadget.com/googles-new-gemini-15-flash-ai-model-is-lighter-than-gemini-pro-and-more-accessible-172353657.html ?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/googles-new-gemini-15-flash-ai-model-is-lighter-than-gemini-pro-and-more-accessible-172353657.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 4aff3560-d8d6-4ebe-ad97-99a0eca939d9 </guid><dc:creator><![CDATA[Pranav Dixit]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:23:53 +0000</pubDate><ingested> 1715707582</ingested><modified> 2024-05-14T17:26:34+00:00 </modified><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Pranav Dixit]]></category><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2Fb659cc90-117c-11ef-afff-712dd6cf20d4&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=8e8095566da487cb1e4caf1f222af87a7e9fe139" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="787" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F938f3e70-117c-11ef-bfe9-3d5d92399b65&amp;resize=1400%2C787&amp;client=19f2b5e49a271b2bde77&amp;signature=282ea88b2035246ce8a5cf3b6dc3157b9691d117" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[Google's brand new AI model is smaller and cheaper for developers to use than Gemini 1.5 Pro]]></media:description><media:title><![CDATA[Google Gemini 1.5 Flash]]></media:title></media:content></item><item><title><![CDATA[Watch the Google I/O 2024 keynote live]]></title><description type="html"><![CDATA[<p><strong>编者注 (5/14/24)</strong> ：Google I/O 主题演讲正在进行中。请参阅下面的直播，并切换到 Engadget <a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-the-latest-on-gemini-ai-android-15-and-more-110008373.html">Google I/O 实时博客</a>进行实时报道。</p><p>又到了每年的这个时候。 Google 的<a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/google-io-2024-takes-place-on-may-14-200705393.html">年度 I/O 主题演讲</a>即将来临。此活动可能会充满更新和公告。我们将报道所有发生的新闻，您可以在下面观看完整的活动。主题演讲于美国东部时间 5 月 14 日下午 1 点开始，<a data-i13n="cpos:3;pos:1" href="https://www.youtube.com/@Google/videos">可通过 YouTube</a>和<a data-i13n="cpos:4;pos:1" href="https://io.google/2024/">公司中心页面</a>观看直播。 </p><span id="end-legacy-contents"></span><div id="989b401cb3084ed7b10954050c3a30ef"><iframe width="560" height="315" src="https://www.youtube.com/embed/XEzRZ35urlk?si=MF25o-HIeMMaEImb" title="YouTube 视频播放器" frameborder="0" allowfullscreen=""></iframe></div><p>就期待而言，谣言工厂<a data-i13n="cpos:5;pos:1" href="https://www.engadget.com/what-to-expect-at-google-io-2024-gemini-android-15-and-more-174535938.html">已经加班加点了</a>。有多个报道称，该活动将主要关注 Android 15 移动操作系统，这似乎是理所当然的，因为 I/O 主要是面向开发者的活动，而且 Beta 版本<a data-i13n="cpos:6;pos:1" href="https://www.yahoo.com/tech/everything-exciting-android-15-beta-194500586.html">已经发布</a>。</p><p>那么让我们来谈谈 Android 15 Beta 版以及对完整版本的期待。该测试版包括更新的隐私沙箱功能、部分屏幕共享以记录某个应用程序或窗口而不是整个屏幕以及系统级应用程序存档以释放空间。还改进了卫星连接、额外的应用内摄像头控制和新的能效模式。</p><p>尽管测试版已经存在，但谷歌很有可能会发布一些令人惊讶的 Android 15 公告。该公司已确认卫星消息功能将登陆 Android，因此这可能会成为本次活动的一部分。有传言还表明， <a data-i13n="cpos:7;pos:1" href="https://www.androidauthority.com/android-15-features-3401939/">Android 15 将拥有重新设计的状态栏</a>和更简单的方法来监控电池健康状况。 </p><figure><img src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/d03eaa70-0e13-11ef-bdde-63c1143a69dc" data-crop-orig-src="https://s.yimg.com/os/creatr-uploaded-images/2024-05/d03eaa70-0e13-11ef-bdde-63c1143a69dc" style="height:1148px;width:1912px;" alt="一部安卓手机。" data-uuid="2e0bfa20-feb3-3595-a0e1-b230abbc922e"><figcaption></figcaption><div class="photo-credit">萨姆·卢瑟福/Engadget</div></figure><p> Android 15 不会是谷歌在活动期间讨论的唯一内容。你可能听说过一个叫做 AI 的小缩写词，该公司已经全力以赴。谷歌很有可能会花相当多的时间宣​​布其 Gemini AI 的更新，它最终可能完全取代 Assistant。</p><p>早在 12 月，就有报道称谷歌正在开发一款<a data-i13n="cpos:8;pos:1" href="https://www.engadget.com/googles-pixel-9-could-arrive-with-a-sophisticated-pixie-ai-assistant-094448458.html">名为 Pixie 的人工智能助手，</a>作为 Pixel 设备的专有功能。品牌肯定是重点。我们可能会听到更多相关消息，因为它可能会在今年晚些时候在 Pixel 9 上首次亮相。</p><p>谷歌最受欢迎的产品也可能得到以人工智能为中心的重新设计，包括搜索、Chrome、G Suite 和地图。我们可能会得到有关该公司计划<a data-i13n="cpos:9;pos:1" href="https://www.engadget.com/google-has-delayed-killing-third-party-cookies-from-chrome-again-155911583.html">对第三方 cookie 采取</a>哪些措施的最新信息，也许它也会在这个问题上投入一些人工智能。</p><p>有什么不值得期待的？不要对这次活动中的 Pixel 9 或更新的 Pixel Fold 抱有希望，因为 I/O 更多的是软件而不是硬件。我们可能会在秋季获得有关这些版本的详细信息。然而，规则是用来打破的。去年，我们<a data-i13n="cpos:10;pos:1" href="https://www.engadget.com/google-pixel-fold-hands-on-finally-a-real-rival-for-samsungs-foldables-185253034.html">在 I/O 大会上宣布了 Pixel Fold</a> ，因此硬件和软件之间的界限可能正在变得模糊。我们很快就会知道。</p>本文最初发表在 Engadget 上：https://www.engadget.com/how-to-watch-googles-io-2024-keynote-160010787.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/how-to-watch-googles-io-2024-keynote-160010787.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> 542feae3-4f2e-422e-820e-e84ca53c22c6</guid><dc:creator><![CDATA[Lawrence Bonk]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:13:19 +0000</pubDate><ingested> 1715706800</ingested><modified> 2024-05-14T17:13:32+00:00 </modified><category><![CDATA[Software]]></category><category><![CDATA[Technology & Electronics]]></category><category><![CDATA[Small Businesses]]></category><category><![CDATA[Information Technology]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Lawrence Bonk]]></category><media:content height="833" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F97e799c0-0e13-11ef-bffb-03274290008c&amp;resize=1400%2C833&amp;client=19f2b5e49a271b2bde77&amp;signature=0fae4e5d1b796b055170a166acf82e99e3d05ea0" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="833" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F97e799c0-0e13-11ef-bffb-03274290008c&amp;resize=1400%2C833&amp;client=19f2b5e49a271b2bde77&amp;signature=5da652dd2fa38063242d6e58f9904b05c6c6cbd0" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[An ad for the chatbot.]]></media:description><media:title><![CDATA[Google Gemini]]></media:title></media:content></item><item><title><![CDATA[Ask Google Photos to help make sense of your gallery]]></title><description type="html"><![CDATA[<p>谷歌正在将更多的 Gemini AI 融入其许多产品中，其下一个目标是照片。在今天的 I/O 开发者大会上，该公司首席执行官 Sundar Pichai 宣布了一项名为“Ask Photos”的功能，该功能旨在帮助您通过与 Gemini 交谈来查找图库中的特定图像。</p><p> “询问照片”将在您的 Google 照片应用底部显示为新选项卡。它将首先向 One 订阅者推出，并在接下来的几个月内以美式英语开始。当您点击该面板时，您会看到双子星图标和栏上方的欢迎消息，提示您“搜索或询问照片”。</p><span id="end-legacy-contents"></span><p>根据谷歌的说法，你可以问诸如“向我展示我访问过的每个国家公园的最佳照片”之类的问题，这不仅会利用 GPS 信息，而且还需要人工智能进行一些判断来确定什么是“最佳”。该公司负责照片的副总裁 Shimrit Ben-Yair 告诉 Engadget，你将能够向人工智能提供反馈，并让它知道你更喜欢哪些照片。 “学习是关键，”本-耶尔说。</p><p>您还可以要求“照片”查找您最近度假时最喜欢的照片，并生成描述它们的标题，以便您可以更快地将它们分享到社交媒体。同样，如果您不喜欢双子座的建议，您也可以稍后进行调整。</p><p>目前，您必须在“询问照片”中输入查询——尚不支持语音输入。随着该功能的推出，那些选择使用它的人将看到他们现有的搜索功能被“升级”为“询问”。然而，谷歌表示，“关键的搜索功能，比如快速访问你的面孔组或地图视图，不会丢失。”</p><p>该公司解释说，“询问照片”流程分为三个部分：“理解你的问题”、“制定回应”和“确保安全并记住更正”。虽然安全只是最后阶段提到的，但应该全程烘焙。该公司承认“您照片中的信息可能非常个人化，我们非常重视保护它的责任。”</p><p>为此，查询不会存储在任何地方，尽管它们是在云中（而不是在设备上）处理的。人们不会在“询问照片”中查看对话或个人数据，除非“在极少数情况下是为了解决虐待或伤害问题”。谷歌还表示，它不会使用这些个人数据训练“谷歌照片之外的任何生成人工智能产品，包括其他 Gemini 模型和产品”。</p><p>您的媒体将继续受到与您使用 Google 相册相同的安全和隐私措施的保护。这是一件好事，因为使用“询问照片”可能更有用的方法之一可能是从您几年前拍摄的照片中获取护照或许可证到期日期等信息。它还使用 Gemini 的多模式功能来读取图像中的文本以找到答案。</p><p>当然，人工智能在谷歌照片中并不是什么新鲜事。您始终能够使用该公司的面部和物体识别算法在应用程序中搜索“信用卡”或特定朋友等内容。但 Gemini AI 带来了生成处理功能，因此照片可以做的不仅仅是提供包含某些人物或物品的照片。</p><p>其他应用程序包括获取照片，以告诉您最近为伴侣或孩子举办的几次生日聚会可能使用了哪些主题。 Gemini AI 正在研究您的照片并找出您已经采用的主题。</p><p> “询问照片”有很多有希望的用例，目前这是一个实验性功能，并且“很快就会开始推出”。与其他照片工具一样，它可能首先作为 One 订阅者和 Pixel 所有者的一项高级功能，然后逐渐普及到所有使用免费应用程序的用户。不过，目前还没有关于何时或是否会发生这种情况的官方消息。</p><p>在<a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/google-io-2024-live-updates-gemini-ai-android-15-and-more-110008966.html"><em>这里</em></a><em>了解 Google I/O 2024 的所有新闻</em><em>！</em></p>本文最初发表在 Engadget 上：https://www.engadget.com/ask-google-photos-to-get-help-making-sense-of-your-gallery-170734062.html?src=rss]]>; </description><link/><![CDATA[https://www.engadget.com/ask-google-photos-to-get-help-making-sense-of-your-gallery-170734062.html?src=rss]]><source_id> engadget_479</source_id><guid ispermalink="false"> f9233213-1e49-45a1-bfcb-7ff3badbc444 </guid><dc:creator><![CDATA[Cherlynn Low]]></dc:creator><source/><![CDATA[Engadget]]><dc:publisher><![CDATA[Engadget]]></dc:publisher><dc:rightsholder><![CDATA[Engadget]]></dc:rightsholder><pubDate> Tue, 14 May 2024 17:08:10 +0000</pubDate><ingested> 1715708182</ingested><modified> 2024-05-14T17:36:36+00:00 </modified><category><![CDATA[Software]]></category><category><![CDATA[site|engadget]]></category><category><![CDATA[provider_name|Engadget]]></category><category><![CDATA[region|US]]></category><category><![CDATA[language|en-US]]></category><category><![CDATA[author_name|Cherlynn Low]]></category><media:content height="789" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fs.yimg.com%2Fos%2Fcreatr-uploaded-images%2F2024-05%2F6b58db50-1186-11ef-beae-d691c441daa3&amp;resize=1400%2C789&amp;client=19f2b5e49a271b2bde77&amp;signature=9ad1bcfe42b2b747868894b9f995e1725531c3e0" width="1400"><media:keywords>标题</media:keywords><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[1]]></dc:identifier><media:credit><![CDATA[ ]]></media:credit><media:description><![CDATA[ ]]></media:description><media:title><![CDATA[ ]]></media:title></media:content><media:content height="789" medium="image" url="https://o.aolcdn.com/images/dims?image_uri=https%3A%2F%2Fmedia-mbst-pub-ue1.s3.amazonaws.com%2Fcreatr-uploaded-images%2F2024-05%2F6b58db50-1186-11ef-beae-d691c441daa3&amp;resize=1400%2C789&amp;client=19f2b5e49a271b2bde77&amp;signature=d115dc7c230f041352d4281d0e511fb7903a037e" width="1400"><media:media_html><![CDATA[ ]]></media:media_html><dc:identifier><![CDATA[2]]></dc:identifier><media:credit><![CDATA[Google]]></media:credit><media:description><![CDATA[A graphic showing a phone with the Gemini star icon and the words "Hi Elisa, What can I help you with?" To the right of the phone are the words "Google Photos" on top of a larger pair of words "Ask Photos" and "with Gemini" in smaller font below it.]]></media:description><media:title><![CDATA[Google Ask Photos]]></media:title></media:content></item></channel></rss>